{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is an exercise notebook for myslf to learn CNN.\n",
    "#### The material I used of this exercise is listed below:\n",
    "    https://machinelearningmastery.com/introduction-to-regularization-to-reduce-overfitting-and-improve-generalization-error/\n",
    "    https://machinelearningmastery.com/early-stopping-to-avoid-overtraining-neural-network-models/\n",
    "    https://machinelearningmastery.com/train-neural-networks-with-noise-to-reduce-overfitting/\n",
    "    https://machinelearningmastery.com/introduction-to-weight-constraints-to-reduce-generalization-error-in-deep-learning/\n",
    "    https://machinelearningmastery.com/weight-regularization-to-reduce-overfitting-of-deep-learning-models/\n",
    "    https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/\n",
    "    https://machinelearningmastery.com/how-to-reduce-overfitting-with-dropout-regularization-in-keras/\n",
    "    Hands on ML chapter 11fgh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Generalization, Underfitting, Overfitting\n",
    "   \n",
    "   ## 1.1 Generalization\n",
    "    The central challenge in machine learning is that we must perform well on new, previously unseen inputs â€” not just those on which our model was trained. The ability to perform well on previously unobserved inputs is called generalization.\n",
    "    We use methods like a train/test split or k-fold cross-validation only to estimate the ability of the model to generalize to new data.\n",
    "   \n",
    "   ## 1.2 Underfitting & Overfitting\n",
    "    Underfit Model: A model with too little capacity cannot learn the problem. An underfit model has high bias and low variance.\n",
    "    Overfit Model: A model with too much capacity can learn it too well and overfit the training dataset. An overfit model has low bias and high variance.The model learns the training data too well and performance varies widely with new unseen examples or even statistical noise added to examples in the training dataset.\n",
    "    Both cases result in a model that does not generalize well. A model that suitably learns the training dataset and generalizes well to the old out dataset.\n",
    "<img src=\"overfitting.png\" alt=\"Drawing\" style=\"width: 400px;\"/> \n",
    "\n",
    "   ## 1.3 Address\n",
    "    Address Underfit: Increase capacity, the ability of a model to fit variety of functions, adding more layers.\n",
    "    Address Underfit: Train the network on more examples OR constrain the complexity of the network. \n",
    "                                                            -by changing the network structure (number of weights)\n",
    "                                                            -by changing the network parameters (values of weights)\n",
    "    Structure: Grid search to find suitable nodes or layers / Remove directly\n",
    "    Patameter: More common. Small parameters suggest a less complex and, in turn, more stable model that is less sensitive to statistical fluctuations in the input data. Large weighs tend to cause sharp transitions in the [activation] functions and thus large changes in output for small changes in inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Regularization\n",
    "    Regularization is any modification we make to a learning algorithm that is intended to reduce its generalization error but not its training error. Regularization is one of the central concerns of the field of machine learning, rivaled in its importance only by optimization.\n",
    "    Modern CNN: use early stopping and dropout, in addition to a weight constraint.\n",
    "    Modern RNN: use early stopping with a backpropagation-through-time-aware version of dropout and a weight constraint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Weight Regularization (weight decay):\n",
    "    Penalize the model during training based on the magnitude of the weights.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Activity Regularization: \n",
    "    Penalize the model during training base on the magnitude of the activations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Weight Constraint: \n",
    "    Constrain the magnitude of weights to be within a range or below a limit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Dropout: \n",
    "    Probabilistically remove inputs during training.\n",
    "\n",
    "### 2.4.1 Keras\n",
    "    https://keras.io/layers/core/\n",
    "    https://machinelearningmastery.com/how-to-reduce-overfitting-with-dropout-regularization-in-keras/\n",
    "    keras.layers.Dropout(rate, noise_shape=None, seed=None)\n",
    "    *rate: float between 0 and 1. Fraction of the input units to drop.\n",
    "    \n",
    "    keras.layers.SpatialDropout1D(rate) / keras.layers.SpatialDropout2D(rate)\n",
    "    This version performs the same function as Dropout, however it drops entire 1D/2D feature maps instead of individual elements.\n",
    "\n",
    "### 2.4.2 Tensorflow\n",
    "    https://www.tensorflow.org/api_docs/python/tf/nn/dropout\n",
    "    https://www.tensorflow.org/tutorials/estimators/cnn\n",
    "    For each element of x, with probability rate, outputs 0, and otherwise scales up the input by 1 / (1-rate). The scaling is such that the expected sum is unchanged.\n",
    "    tf.nn.dropout(x, keep_prob=None, noise_shape=None, seed=None, name=None, rate=None)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify MINIST using CNN\n",
    "# Architecture would be 1C, 2P, 3C, 4P, 5D, 5D\n",
    "\n",
    "# import packages\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model, including architectur, predictions, loss, tranining, evaluation\n",
    "def cnn_model(features, labels, mode):\n",
    "    # Input layer\n",
    "    input_layer  = tf.reshape(features['X'], [-1, 28, 28,1]) # -1 here basically means: I do not have time to calculate all the dimensions, so infer the one for me. Because x * 28 * 28 * 1 = 784, so  -1 = 1\n",
    "    \n",
    "    # convolutional layer C1\n",
    "    conv1 = tf.layers.conv2d(\n",
    "        inputs = input_layer,\n",
    "        filters = 32,\n",
    "        kernel_size = [5, 5],\n",
    "        padding = 'same',\n",
    "        activation = tf.nn.relu\n",
    "    )\n",
    "    \n",
    "    # pooling layer P2\n",
    "    pool1 = tf.layers.max_pooling2d(inputs = conv1,\n",
    "                                    pool_size = [2, 2],\n",
    "                                    strides = 2\n",
    "                                   ) # Max pooling layer for 2D inputs (e.g. images). This will be deprecated in future version. Use keras.layers.MaxPooling2D instead\n",
    "    \n",
    "    # convolutional layer C3\n",
    "    conv2 = tf.layers.conv2d(\n",
    "    inputs = pool1,\n",
    "    filters = 64,\n",
    "    kernel_size = [5, 5],\n",
    "    padding = 'same',\n",
    "    activation = tf.nn.relu)\n",
    "    \n",
    "    # pooling layer P4\n",
    "    pool2 = tf.layers.max_pooling2d(inputs = conv2,\n",
    "                                    pool_size = [2, 2],\n",
    "                                    strides = 2\n",
    "                                   )\n",
    "    \n",
    "    # dense layer\n",
    "    pool2_flat = tf.reshape(pool2, [-1,7*7*64]) # breakds the spatial structure of the data and transforms tridimensional tensor into a monodimensional tensor(a vector)\n",
    "    dense = tf.layers.dense(inputs = pool2_flat, \n",
    "                            units = 2014, \n",
    "                            activation = tf.nn.relu)\n",
    "    dropout = tf.layers.dropout(inputs = dense, \n",
    "                                rate = 0.4, \n",
    "                                training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    \n",
    "    # logits layer\n",
    "    logits = tf.layers.dense(inputs = dropout, units =10) # 10 classes for MINIST classification\n",
    "    \n",
    "    predictions = {\n",
    "        # generate predictions of the class\n",
    "        'classes': tf.argmax(input = logits, axis = 1), # Returns the index with the largest value across axes of a tensor\n",
    "        # print the p of predictions\n",
    "        'probilities': tf.nn.softmax(logits, name = 'softmax_tensor')\n",
    "    }\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.PREDICT: # standard keys are defined: TRAIN, EVAL, PREDICT\n",
    "        return tf.estimator.EstimatorSpec(mode = mode, \n",
    "                                          predictions = predictions)\n",
    "    \n",
    "    \n",
    "    # calculate loss\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels = labels,\n",
    "                                                 logits = logits)\n",
    "    \n",
    "    # configure the training Op\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimize = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "        train_op = optimize.minimize(loss = loss, # training optimization operation, add operations to minimize loss by updating var_list\n",
    "                                    global_step = tf.train.get_global_step()) # The global step variable\n",
    "        return tf.estimator.EstimatorSpec(mode = mode, \n",
    "                                          loss = loss, \n",
    "                                          train_op = train_op)\n",
    "    \n",
    "    # add evaluation metrics\n",
    "    eval_metric_ops = {\n",
    "        'accuracy': tf.metrics.accuracy(labels = labels,\n",
    "                                       predictions = predictions['classes'])\n",
    "    }\n",
    "    return tf.estimator.EstimatorSpec(mode = mode,\n",
    "                                     loss = loss,\n",
    "                                     eval_metric_ops = eval_metric_ops)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train and eval data\n",
    "((train_data, train_labels), (eval_data, eval_labels)) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# format input data\n",
    "train_data = train_data/np.float32(225)\n",
    "train_labels = train_labels.astype(np.int32)\n",
    "\n",
    "eval_data = eval_data/np.float32(255)\n",
    "eval_labels = eval_labels.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I0803 08:32:59.268401 4590163392 estimator.py:1790] Using default config.\n",
      "I0803 08:32:59.271323 4590163392 estimator.py:209] Using config: {'_model_dir': '/tmp/mnist_convnet_model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x6300196a0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "# create estimator\n",
    "mnist_classifier = tf.estimator.Estimator(model_fn = cnn_model,\n",
    "                                         model_dir = \"/tmp/mnist_convnet_model\"\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up logging to track progress during training\n",
    "tensors_to_log = {'probilities':'softmax_tensor'}\n",
    "logging_hook = tf.train.LoggingTensorHook(\n",
    "    tensors = tensors_to_log, every_n_iter = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0803 08:32:59.307542 4590163392 deprecation.py:323] From /Users/feifan/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "W0803 08:32:59.376926 4590163392 deprecation.py:323] From /Users/feifan/anaconda3/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_queue_runner.py:62: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "W0803 08:32:59.378787 4590163392 deprecation.py:323] From /Users/feifan/anaconda3/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_functions.py:500: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "I0803 08:32:59.391939 4590163392 estimator.py:1145] Calling model_fn.\n",
      "W0803 08:32:59.394644 4590163392 deprecation.py:323] From <ipython-input-2-98aa3dcb0d7d>:12: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2D` instead.\n",
      "W0803 08:32:59.398087 4590163392 deprecation.py:506] From /Users/feifan/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0803 08:32:59.583414 4590163392 deprecation.py:323] From <ipython-input-2-98aa3dcb0d7d>:18: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.MaxPooling2D instead.\n",
      "W0803 08:32:59.705687 4590163392 deprecation.py:323] From <ipython-input-2-98aa3dcb0d7d>:39: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "W0803 08:32:59.952198 4590163392 deprecation.py:323] From <ipython-input-2-98aa3dcb0d7d>:42: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n",
      "W0803 08:33:00.026659 4590163392 deprecation.py:323] From /Users/feifan/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/losses/losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "I0803 08:33:00.110388 4590163392 estimator.py:1147] Done calling model_fn.\n",
      "I0803 08:33:00.113330 4590163392 basic_session_run_hooks.py:541] Create CheckpointSaverHook.\n",
      "I0803 08:33:00.258684 4590163392 monitored_session.py:240] Graph was finalized.\n",
      "W0803 08:33:00.262720 4590163392 deprecation.py:323] From /Users/feifan/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "I0803 08:33:00.265120 4590163392 saver.py:1280] Restoring parameters from /tmp/mnist_convnet_model/model.ckpt-1001\n",
      "W0803 08:33:00.336631 4590163392 deprecation.py:323] From /Users/feifan/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1066: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file utilities to get mtimes.\n",
      "I0803 08:33:00.355098 4590163392 session_manager.py:500] Running local_init_op.\n",
      "I0803 08:33:00.361896 4590163392 session_manager.py:502] Done running local_init_op.\n",
      "W0803 08:33:00.376370 4590163392 deprecation.py:323] From /Users/feifan/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py:875: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "I0803 08:33:00.523838 4590163392 basic_session_run_hooks.py:606] Saving checkpoints for 1001 into /tmp/mnist_convnet_model/model.ckpt.\n",
      "I0803 08:33:00.900053 4590163392 basic_session_run_hooks.py:262] probilities = [[0.08246178 0.10371523 0.17724669 0.10736182 0.08892082 0.08452033\n",
      "  0.08325318 0.07078844 0.13952065 0.06221103]\n",
      " [0.11784818 0.04064286 0.16658983 0.04612596 0.07950204 0.09101323\n",
      "  0.18370381 0.09059454 0.08925875 0.09472081]\n",
      " [0.25192332 0.02527721 0.09064966 0.12736094 0.06528188 0.09668827\n",
      "  0.12259277 0.085972   0.06771541 0.0665386 ]\n",
      " [0.08974385 0.06948555 0.23581833 0.1218602  0.08010177 0.06339121\n",
      "  0.11317565 0.0607472  0.08693513 0.07874105]\n",
      " [0.09352668 0.07864743 0.11768793 0.10038107 0.08934674 0.05549154\n",
      "  0.07078158 0.14198348 0.13672858 0.11542495]\n",
      " [0.06789827 0.01759493 0.22393553 0.06371414 0.07973568 0.05085428\n",
      "  0.25167653 0.09610736 0.0861446  0.06233878]\n",
      " [0.09559159 0.09175064 0.07463514 0.10327327 0.10048139 0.09296016\n",
      "  0.18695326 0.06689173 0.11349475 0.0739681 ]\n",
      " [0.09086476 0.08134679 0.10866573 0.10451891 0.06974872 0.09487098\n",
      "  0.08342599 0.10226028 0.19218647 0.07211145]\n",
      " [0.12118433 0.07351324 0.17504081 0.09787139 0.11491122 0.07159014\n",
      "  0.08779775 0.04439314 0.15653968 0.05715817]\n",
      " [0.08617295 0.06037139 0.08528021 0.18636566 0.12925492 0.10751206\n",
      "  0.09176245 0.05836242 0.1159004  0.07901753]\n",
      " [0.2181961  0.0394727  0.04556574 0.09566636 0.05943848 0.11818492\n",
      "  0.12143493 0.11926591 0.09895746 0.08381741]\n",
      " [0.08671694 0.08169448 0.13831696 0.12666346 0.08141363 0.07592313\n",
      "  0.1363262  0.05645068 0.13946274 0.07703182]\n",
      " [0.08365159 0.06776215 0.07560331 0.11800452 0.10681556 0.10914072\n",
      "  0.10995395 0.08643027 0.0841651  0.15847287]\n",
      " [0.12798522 0.04554395 0.07189038 0.13214989 0.11802175 0.08933093\n",
      "  0.10810736 0.06572974 0.12024543 0.12099525]\n",
      " [0.14086096 0.065368   0.07469646 0.10585999 0.08570684 0.1170055\n",
      "  0.07017889 0.15502207 0.08395053 0.10135084]\n",
      " [0.1494936  0.09580617 0.08190284 0.12088902 0.07683605 0.09751724\n",
      "  0.07053486 0.10688119 0.10354219 0.09659686]\n",
      " [0.12220547 0.05005385 0.06111997 0.07571324 0.12273671 0.06090313\n",
      "  0.05683726 0.18714556 0.08427872 0.17900608]\n",
      " [0.09149271 0.05495853 0.10226831 0.08636697 0.1948268  0.08788653\n",
      "  0.09808325 0.08716809 0.09356425 0.1033847 ]\n",
      " [0.09178504 0.06706827 0.10827439 0.09346964 0.09739462 0.09411857\n",
      "  0.14959231 0.07270744 0.12286231 0.10272739]\n",
      " [0.09300464 0.03652228 0.07551696 0.08639619 0.17053333 0.09465047\n",
      "  0.08618668 0.12683731 0.08843769 0.14191438]\n",
      " [0.09880333 0.05382068 0.12411243 0.08521174 0.10056562 0.0777832\n",
      "  0.16550902 0.10465692 0.10292642 0.08661066]\n",
      " [0.09770828 0.05456988 0.07755864 0.08595192 0.12378788 0.09480464\n",
      "  0.08304362 0.13224113 0.11065345 0.13968061]\n",
      " [0.15482685 0.05767073 0.24313642 0.09807254 0.08329581 0.08552273\n",
      "  0.0927928  0.03525538 0.09557843 0.05384832]\n",
      " [0.16032402 0.04779696 0.11036479 0.08935631 0.08691463 0.10425679\n",
      "  0.15494287 0.05006997 0.10463817 0.09133548]\n",
      " [0.04943172 0.23824815 0.09051078 0.15409665 0.07279338 0.06041202\n",
      "  0.08031422 0.08815171 0.10547464 0.0605667 ]\n",
      " [0.10397717 0.03817752 0.14263731 0.19432168 0.06413951 0.07670401\n",
      "  0.13806364 0.03350883 0.17745908 0.03101131]\n",
      " [0.11752855 0.05062961 0.07980607 0.07773064 0.10609862 0.07831495\n",
      "  0.06894433 0.18805969 0.09754948 0.13533804]\n",
      " [0.13165954 0.06069054 0.07516035 0.10283738 0.0948284  0.09385011\n",
      "  0.08501504 0.13415915 0.09387078 0.12792869]\n",
      " [0.07934985 0.05841833 0.16358623 0.0683853  0.11236715 0.11440766\n",
      "  0.15427318 0.06354199 0.09819391 0.08747641]\n",
      " [0.07224867 0.09005498 0.1088944  0.10630295 0.14368038 0.08989916\n",
      "  0.09402863 0.07015178 0.13029298 0.09444599]\n",
      " [0.20251808 0.07830288 0.14400807 0.07733237 0.07265411 0.07213609\n",
      "  0.1289281  0.07812648 0.07296811 0.07302571]\n",
      " [0.1348506  0.04451685 0.12628932 0.11956441 0.09469331 0.08165514\n",
      "  0.1257841  0.09507339 0.08838765 0.08918526]\n",
      " [0.10242087 0.05732786 0.07788084 0.19101295 0.06857559 0.12561838\n",
      "  0.07659905 0.10500289 0.132575   0.06298646]\n",
      " [0.12349206 0.079466   0.12829679 0.11705284 0.0540516  0.06962981\n",
      "  0.08837974 0.13709384 0.11783625 0.08470108]\n",
      " [0.07306076 0.1293675  0.07342418 0.1206504  0.1031273  0.08153651\n",
      "  0.04784143 0.14147238 0.1281892  0.10133029]\n",
      " [0.09742931 0.08022568 0.10366971 0.18253422 0.06752647 0.08307543\n",
      "  0.0834173  0.11120564 0.11549573 0.07542055]\n",
      " [0.09901563 0.05916352 0.31338993 0.11388457 0.06691366 0.06425508\n",
      "  0.09120955 0.04140402 0.11827782 0.03248624]\n",
      " [0.12488115 0.06027575 0.09508742 0.09993044 0.0956535  0.06107419\n",
      "  0.06700272 0.20511508 0.10388803 0.08709162]\n",
      " [0.11486166 0.05896325 0.15200378 0.10706338 0.07845467 0.09413727\n",
      "  0.1484562  0.04931837 0.13774697 0.05899446]\n",
      " [0.07891235 0.08569799 0.0806661  0.09925374 0.09891941 0.08530603\n",
      "  0.06123208 0.16057114 0.10683022 0.14261083]\n",
      " [0.09250059 0.1005271  0.10431561 0.08533068 0.07207676 0.12212247\n",
      "  0.07581946 0.15333202 0.08429952 0.10967577]\n",
      " [0.07640021 0.11562864 0.12344307 0.12659359 0.09743028 0.06621647\n",
      "  0.08069304 0.08429373 0.16903327 0.06026779]\n",
      " [0.09042296 0.09441417 0.09072271 0.09614381 0.09095211 0.08965001\n",
      "  0.08182421 0.10911065 0.12922892 0.12753043]\n",
      " [0.05447882 0.0910005  0.19973762 0.12369464 0.09419851 0.07562803\n",
      "  0.12797558 0.04232469 0.13477863 0.05618294]\n",
      " [0.08470448 0.06465641 0.09080429 0.07034907 0.1621725  0.09904402\n",
      "  0.11472151 0.0992741  0.10293977 0.11133382]\n",
      " [0.07822193 0.07533129 0.12348346 0.0963859  0.07927088 0.08865804\n",
      "  0.12353455 0.06963178 0.15876272 0.10671947]\n",
      " [0.10254689 0.07777799 0.12667781 0.12450725 0.10023453 0.062748\n",
      "  0.09470063 0.08126536 0.14375365 0.08578793]\n",
      " [0.21697865 0.04953982 0.08389654 0.14382146 0.07753079 0.11576201\n",
      "  0.09147596 0.05925693 0.10815235 0.05358555]\n",
      " [0.07046704 0.06071254 0.11556858 0.11508183 0.0679331  0.08376497\n",
      "  0.07221381 0.19288012 0.0995046  0.12187343]\n",
      " [0.11599126 0.06254774 0.12847434 0.05813694 0.148409   0.0510061\n",
      "  0.06315622 0.12519728 0.12231597 0.12476514]\n",
      " [0.06391294 0.18194869 0.09784155 0.11409112 0.10625398 0.084616\n",
      "  0.07780982 0.06506153 0.14735064 0.06111382]\n",
      " [0.05770736 0.14954545 0.0929267  0.09068632 0.09352255 0.0764076\n",
      "  0.09354016 0.09707458 0.1393289  0.10926047]\n",
      " [0.07060803 0.08787322 0.08939022 0.1659496  0.1169913  0.07924653\n",
      "  0.06690541 0.08717156 0.1865842  0.04927991]\n",
      " [0.09824431 0.07811614 0.10315488 0.09444022 0.09826946 0.05490291\n",
      "  0.19874473 0.04671654 0.14748469 0.07992611]\n",
      " [0.19520284 0.02131869 0.0839536  0.05120765 0.16930805 0.08546235\n",
      "  0.08639307 0.10935144 0.07462329 0.12317903]\n",
      " [0.08691033 0.04099647 0.08514524 0.0972518  0.2093077  0.07560469\n",
      "  0.0720106  0.13480414 0.08325819 0.11471082]\n",
      " [0.14582133 0.02656206 0.10289846 0.15625261 0.0561417  0.04773667\n",
      "  0.05526159 0.21411061 0.09496355 0.1002515 ]\n",
      " [0.11251131 0.06382668 0.11428212 0.10959346 0.06043454 0.08640113\n",
      "  0.21949567 0.04888633 0.10025142 0.08431739]\n",
      " [0.12438167 0.07059317 0.12714544 0.16440976 0.03652162 0.11166498\n",
      "  0.07570456 0.09309699 0.13293941 0.06354239]\n",
      " [0.14656447 0.08869755 0.09882025 0.09362451 0.10215769 0.08227833\n",
      "  0.080687   0.14403597 0.09649863 0.06663562]\n",
      " [0.10716992 0.01746633 0.16800562 0.08868826 0.16592771 0.07283755\n",
      "  0.15038958 0.11169443 0.06720244 0.05061817]\n",
      " [0.06539345 0.07536919 0.11321259 0.08414757 0.15731135 0.05576188\n",
      "  0.06608615 0.12089349 0.11500748 0.14681678]\n",
      " [0.06658355 0.1934179  0.11871234 0.11128891 0.10320424 0.07698151\n",
      "  0.08746047 0.08582293 0.0896631  0.06686506]\n",
      " [0.06504251 0.1778158  0.09717855 0.13937326 0.09009324 0.06585809\n",
      "  0.0819941  0.0645097  0.1454818  0.0726529 ]\n",
      " [0.16742592 0.06091534 0.08091471 0.12999958 0.08168972 0.11492223\n",
      "  0.0875147  0.08600066 0.13856235 0.05205488]\n",
      " [0.14334413 0.03843523 0.11749839 0.07170366 0.15005292 0.08123679\n",
      "  0.07378584 0.0937648  0.08401025 0.14616786]\n",
      " [0.15719609 0.04671116 0.12325966 0.21086255 0.05920383 0.08775364\n",
      "  0.12301241 0.05002277 0.09334119 0.04863679]\n",
      " [0.08582579 0.05732527 0.2431558  0.14085993 0.08852624 0.06348296\n",
      "  0.10116785 0.04491387 0.11565372 0.05908852]\n",
      " [0.10786017 0.10193992 0.09139565 0.10368548 0.08305619 0.08454073\n",
      "  0.08166925 0.11119369 0.14977643 0.0848825 ]\n",
      " [0.14219384 0.0301413  0.11357524 0.16501708 0.08058173 0.06047718\n",
      "  0.10218332 0.06381442 0.12803552 0.11398032]\n",
      " [0.07817399 0.07807551 0.09634791 0.08406017 0.13205045 0.08792976\n",
      "  0.10039038 0.11177598 0.13086566 0.10033015]\n",
      " [0.07212704 0.08543623 0.09186692 0.07606758 0.14032808 0.08153936\n",
      "  0.09918339 0.10747504 0.12873332 0.11724297]\n",
      " [0.05103776 0.0764075  0.08566813 0.2191286  0.04561834 0.11986747\n",
      "  0.08903023 0.10761188 0.11165646 0.09397367]\n",
      " [0.20140985 0.05383464 0.13183583 0.09256298 0.09852906 0.08583699\n",
      "  0.11321444 0.07864339 0.09659312 0.04753973]\n",
      " [0.09348649 0.08400067 0.06735672 0.10731681 0.12407544 0.09011883\n",
      "  0.08759428 0.11487973 0.1182309  0.11294012]\n",
      " [0.17524904 0.03890269 0.08946504 0.11262714 0.05527809 0.08793647\n",
      "  0.14221871 0.06900349 0.13101992 0.09829949]\n",
      " [0.316252   0.02824466 0.07568425 0.1039395  0.04728674 0.12804307\n",
      "  0.06493851 0.05330168 0.10594077 0.07636879]\n",
      " [0.10701672 0.09101563 0.06681759 0.17075494 0.09438635 0.08175815\n",
      "  0.08215699 0.11911198 0.10083826 0.08614341]\n",
      " [0.26585633 0.02357907 0.08936714 0.17799598 0.03724067 0.08375766\n",
      "  0.07211238 0.06512817 0.12736464 0.05759799]\n",
      " [0.08356649 0.11448772 0.09955543 0.18023059 0.06084381 0.08201882\n",
      "  0.12411583 0.05868728 0.1419102  0.05458377]\n",
      " [0.14091003 0.06574029 0.05846141 0.08575782 0.0780812  0.05266016\n",
      "  0.10030568 0.08649276 0.23153177 0.10005888]\n",
      " [0.05540377 0.0432194  0.20172028 0.18499407 0.0465844  0.0550129\n",
      "  0.09786285 0.08469385 0.1623424  0.06816607]\n",
      " [0.09476909 0.09583396 0.0942698  0.09271069 0.0990108  0.06633267\n",
      "  0.09215818 0.12274184 0.12943639 0.11273662]\n",
      " [0.2855112  0.02437106 0.11334112 0.1336434  0.06472117 0.08217675\n",
      "  0.07032218 0.04606096 0.12102529 0.05882686]\n",
      " [0.05632424 0.26025006 0.1007209  0.09217923 0.08372044 0.07086635\n",
      "  0.07006343 0.0548734  0.16003676 0.05096523]\n",
      " [0.05658443 0.23366377 0.09026883 0.10205715 0.07260332 0.06175544\n",
      "  0.08004014 0.08154679 0.16557483 0.05590528]\n",
      " [0.09088179 0.08034831 0.06781273 0.09555501 0.12209692 0.07471485\n",
      "  0.06792766 0.12731408 0.14910863 0.12424002]\n",
      " [0.0524094  0.1828775  0.1138707  0.11285032 0.08161379 0.10320188\n",
      "  0.08022921 0.06441212 0.14371186 0.06482334]\n",
      " [0.10304125 0.06731676 0.11217158 0.09884078 0.10625062 0.09293468\n",
      "  0.09979006 0.11702798 0.13460313 0.06802312]\n",
      " [0.06291704 0.28989625 0.08273034 0.11676297 0.05433525 0.08247293\n",
      "  0.08791828 0.08131541 0.09265205 0.04899941]\n",
      " [0.07829171 0.19573835 0.10512072 0.1188102  0.09732062 0.07041096\n",
      "  0.07211167 0.07955666 0.11863768 0.0640014 ]\n",
      " [0.14287737 0.07345638 0.06459149 0.09514862 0.08836388 0.0922933\n",
      "  0.06459425 0.13994914 0.12162013 0.11710551]\n",
      " [0.0828573  0.09555793 0.12719734 0.09309386 0.10326333 0.09470876\n",
      "  0.1605514  0.05153694 0.12334636 0.06788675]\n",
      " [0.13138486 0.07835021 0.08383065 0.10986327 0.0713136  0.08813722\n",
      "  0.09865598 0.10226866 0.12317968 0.11301588]\n",
      " [0.10412871 0.0673686  0.07894253 0.07448147 0.09001002 0.11602709\n",
      "  0.10231387 0.13975532 0.1181221  0.10885027]\n",
      " [0.08249088 0.03665175 0.11347587 0.05886779 0.17617409 0.0709797\n",
      "  0.09306233 0.13780825 0.10902093 0.12146849]\n",
      " [0.12675689 0.03747591 0.07506639 0.05049888 0.178367   0.10325951\n",
      "  0.12369748 0.08724657 0.08763705 0.12999432]\n",
      " [0.09147736 0.07400451 0.05917039 0.13353616 0.14559416 0.09970866\n",
      "  0.05378845 0.11268795 0.09151768 0.13851474]\n",
      " [0.10365615 0.02426869 0.04728088 0.0716307  0.11004341 0.07521884\n",
      "  0.05544598 0.28076056 0.07562932 0.15606543]\n",
      " [0.1171101  0.05854077 0.06500076 0.10532676 0.09926052 0.08759387\n",
      "  0.08022096 0.151064   0.10888893 0.12699328]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0803 08:33:00.901278 4590163392 basic_session_run_hooks.py:262] loss = 1.8128418, step = 1002\n",
      "I0803 08:33:00.902952 4590163392 basic_session_run_hooks.py:606] Saving checkpoints for 1002 into /tmp/mnist_convnet_model/model.ckpt.\n",
      "W0803 08:33:00.989573 4590163392 deprecation.py:323] From /Users/feifan/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "I0803 08:33:01.164240 4590163392 estimator.py:368] Loss for final step: 1.8128418.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow_estimator.python.estimator.estimator.Estimator at 0x10a5e48d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the Model\n",
    "train_input = tf.estimator.inputs.numpy_input_fn(\n",
    "    x = {'X':train_data},\n",
    "    y = train_labels,\n",
    "    batch_size = 100,\n",
    "    num_epochs = None,\n",
    "    shuffle = True # Avoid shuffle at prediction time.\n",
    ") # Returns input function that would feed dict of numpy arrays into the model.\n",
    "\n",
    "# train one step and display the probabilities\n",
    "mnist_classifier.train(input_fn = train_input, \n",
    "                       steps = 1, \n",
    "                       hooks = [logging_hook])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0803 08:33:01.211869 4590163392 estimator.py:1145] Calling model_fn.\n",
      "I0803 08:33:01.512956 4590163392 estimator.py:1147] Done calling model_fn.\n",
      "I0803 08:33:01.516400 4590163392 basic_session_run_hooks.py:541] Create CheckpointSaverHook.\n",
      "I0803 08:33:01.607152 4590163392 monitored_session.py:240] Graph was finalized.\n",
      "I0803 08:33:01.609923 4590163392 saver.py:1280] Restoring parameters from /tmp/mnist_convnet_model/model.ckpt-1002\n",
      "I0803 08:33:01.659487 4590163392 session_manager.py:500] Running local_init_op.\n",
      "I0803 08:33:01.665563 4590163392 session_manager.py:502] Done running local_init_op.\n",
      "I0803 08:33:01.852787 4590163392 basic_session_run_hooks.py:606] Saving checkpoints for 1002 into /tmp/mnist_convnet_model/model.ckpt.\n",
      "I0803 08:33:02.188526 4590163392 basic_session_run_hooks.py:262] loss = 1.8116452, step = 1003\n",
      "I0803 08:33:15.314663 4590163392 basic_session_run_hooks.py:692] global_step/sec: 7.61822\n",
      "I0803 08:33:15.315727 4590163392 basic_session_run_hooks.py:260] loss = 1.6459488, step = 1103 (13.127 sec)\n",
      "I0803 08:33:29.594712 4590163392 basic_session_run_hooks.py:692] global_step/sec: 7.00278\n",
      "I0803 08:33:29.595932 4590163392 basic_session_run_hooks.py:260] loss = 1.4658812, step = 1203 (14.280 sec)\n",
      "I0803 08:33:43.910362 4590163392 basic_session_run_hooks.py:692] global_step/sec: 6.9854\n",
      "I0803 08:33:43.911694 4590163392 basic_session_run_hooks.py:260] loss = 1.36104, step = 1303 (14.316 sec)\n",
      "I0803 08:33:56.798800 4590163392 basic_session_run_hooks.py:692] global_step/sec: 7.75885\n",
      "I0803 08:33:56.800505 4590163392 basic_session_run_hooks.py:260] loss = 1.2587528, step = 1403 (12.889 sec)\n",
      "I0803 08:34:10.491702 4590163392 basic_session_run_hooks.py:692] global_step/sec: 7.30305\n",
      "I0803 08:34:10.493387 4590163392 basic_session_run_hooks.py:260] loss = 0.9480785, step = 1503 (13.693 sec)\n",
      "I0803 08:34:24.284168 4590163392 basic_session_run_hooks.py:692] global_step/sec: 7.25037\n",
      "I0803 08:34:24.285519 4590163392 basic_session_run_hooks.py:260] loss = 0.7596153, step = 1603 (13.792 sec)\n",
      "I0803 08:34:42.122373 4590163392 basic_session_run_hooks.py:692] global_step/sec: 5.60596\n",
      "I0803 08:34:42.124744 4590163392 basic_session_run_hooks.py:260] loss = 0.6525794, step = 1703 (17.839 sec)\n",
      "I0803 08:34:55.144881 4590163392 basic_session_run_hooks.py:692] global_step/sec: 7.67895\n",
      "I0803 08:34:55.145949 4590163392 basic_session_run_hooks.py:260] loss = 0.6557551, step = 1803 (13.021 sec)\n",
      "I0803 08:35:14.346217 4590163392 basic_session_run_hooks.py:692] global_step/sec: 5.20798\n",
      "I0803 08:35:14.348329 4590163392 basic_session_run_hooks.py:260] loss = 0.6994575, step = 1903 (19.202 sec)\n",
      "I0803 08:35:34.549316 4590163392 basic_session_run_hooks.py:606] Saving checkpoints for 2002 into /tmp/mnist_convnet_model/model.ckpt.\n",
      "I0803 08:35:34.790215 4590163392 estimator.py:368] Loss for final step: 0.5697871.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow_estimator.python.estimator.estimator.Estimator at 0x10a5e48d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model longer\n",
    "# Training CNNs is computationally intensive. \n",
    "# To increase the accuracy of your model, increase the number of steps passed to train(), like 20,000 steps.\n",
    "mnist_classifier.train(input_fn=train_input, steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0803 08:35:34.846904 4590163392 estimator.py:1145] Calling model_fn.\n",
      "I0803 08:35:34.966660 4590163392 estimator.py:1147] Done calling model_fn.\n",
      "I0803 08:35:34.987770 4590163392 evaluation.py:255] Starting evaluation at 2019-08-03T08:35:34Z\n",
      "I0803 08:35:35.068368 4590163392 monitored_session.py:240] Graph was finalized.\n",
      "I0803 08:35:35.070278 4590163392 saver.py:1280] Restoring parameters from /tmp/mnist_convnet_model/model.ckpt-2002\n",
      "I0803 08:35:35.115182 4590163392 session_manager.py:500] Running local_init_op.\n",
      "I0803 08:35:35.125952 4590163392 session_manager.py:502] Done running local_init_op.\n",
      "I0803 08:35:38.260951 4590163392 evaluation.py:275] Finished evaluation at 2019-08-03-08:35:38\n",
      "I0803 08:35:38.261798 4590163392 estimator.py:2039] Saving dict for global step 2002: accuracy = 0.8747, global_step = 2002, loss = 0.5693401\n",
      "I0803 08:35:38.307658 4590163392 estimator.py:2099] Saving 'checkpoint_path' summary for global step 2002: /tmp/mnist_convnet_model/model.ckpt-2002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.8747, 'loss': 0.5693401, 'global_step': 2002}\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model, test accuracy on the Mnist test set\n",
    "\n",
    "eval_input = tf.estimator.inputs.numpy_input_fn(\n",
    "    x = {'X':eval_data},\n",
    "    y = eval_labels,\n",
    "    num_epochs = 1,\n",
    "    shuffle = False\n",
    ")\n",
    "\n",
    "eval_results = mnist_classifier.evaluate(input_fn = eval_input)\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Noise: \n",
    "    Add statistical noise to inputs during training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Early Stopping: \n",
    "    Monitor model performance on a validation set and stop training when performance degrades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
