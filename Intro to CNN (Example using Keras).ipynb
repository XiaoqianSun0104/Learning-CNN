{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is an exercise notebook for myslf to learn CNN.\n",
    "#### The material I used of this exercise is listed below:\n",
    "    https://machinelearningmastery.com/convolutional-layers-for-deep-learning-neural-networks/\n",
    "    https://machinelearningmastery.com/pooling-layers-for-convolutional-neural-networks/\n",
    "    https://machinelearningmastery.com/review-of-architectural-innovations-for-convolutional-neural-networks-for-image-classification/\n",
    "    https://machinelearningmastery.com/padding-and-stride-for-convolutional-neural-networks/\n",
    "    https://machinelearningmastery.com/review-of-architectural-innovations-for-convolutional-neural-networks-for-image-classification/\n",
    "    http://dataaspirant.com/2017/03/07/difference-between-softmax-function-and-sigmoid-function/\n",
    "    https://medium.com/data-science-bootcamp/understand-the-softmax-function-in-minutes-f3a59641e86d\n",
    "    https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/\n",
    "    Hands on Machine Learning, Chapter 13."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional layers:\n",
    "## 1.Convolutional\n",
    "    CNN is a type of neural network that is designed to deal with two-dimensional image data, although it can be used to deal with one-dimensional or three-dimensional data. \n",
    "    The convolutional layer that performs an operation called 'convolution' is central to CNN.\n",
    "    A a convolution is a linear operation that involves the multiplication of a set of weights with the input. The multiplication is performed between an array of input data and a two-dimensional array of weights, called a filter or a kernel.\n",
    "    \n",
    "## 2.Feature Map\n",
    "    Repeated application of the same filter to an input results in a map of activations called a feature map, indicating the locations and strength of a detected feature in an input, such as an image.\n",
    "    The output from multiplying the filter with the input array one time is a single value. As the filter is applied multiple times to the input array, the result is a two-dimensional array of output values that represent a filtering of the input. As such, the two-dimensional output array from this operation is called a “feature map“.\n",
    "    In summary, we have a input, such as an image of pixel values, and we have a filter, which is a set of weights, and the filter is systematically applied to the input data to create a feature map.\n",
    "    \n",
    "## 3. Weights\n",
    "    The innovation of using the convolution operation in a neural network is that the values of the filter are weights to be learned during the training of the network.\n",
    "    The filter weights represent the structure or feature that the filter will detect and the strength of the activation indicates the degree to which the feature was detected.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Powerful:\n",
    "## 1.Multiple features:\n",
    "    Convolutional neural networks do not learn a single filter; they, in fact, learn multiple features in parallel for a given input.\n",
    "    It is common for a convolutional layer to learn from 32 to 512 filters in parallel for a given input.This gives the model 32, or even 512, different ways of extracting features from an input, or many different ways of both “learning to see” and after training, many different ways of “seeing” the input data.\n",
    "    \n",
    "## 2.Multiple channels:\n",
    "    Color images have multiple channels, typically one for each color channel, such as red, green, and blue.\n",
    "    A filter must always have the same number of channels as the input, often referred to as “depth“. \n",
    "    \n",
    "## 3.Multiple layers:\n",
    "    Convolutional layers are not only applied to input data, but they can also be applied to the output of other layers.\n",
    "    Lower layers extract low-level features, such as lines. Very deep layers are extracting faces, animals, houses, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Dimensional input data example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import asarray\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[[-0.9488809 ]],\n",
      "\n",
      "       [[-0.66656923]],\n",
      "\n",
      "       [[ 0.1292522 ]]], dtype=float32), array([0.], dtype=float32)]\n",
      "[[[ 0.        ]\n",
      "  [ 0.1292522 ]\n",
      "  [-0.53731704]\n",
      "  [-1.6154501 ]\n",
      "  [-0.9488809 ]\n",
      "  [ 0.        ]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Note that the feature map has six elements, whereas our input has eight elements.\\nWe can use padding to get 8 elements feature map.\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define input data\n",
    "data = asarray([0, 0, 0, 1, 1, 0, 0, 0])\n",
    "data = data.reshape(1, 8, 1)\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Conv1D(1, 3, input_shape=(8, 1)))\n",
    "# define a vertical line detector\n",
    "weights = [asarray([[[0]],[[1]],[[0]]]), asarray([0.0])]\n",
    "# confirm they were stored\n",
    "print(model.get_weights())\n",
    "# apply filter to input data\n",
    "yhat = model.predict(data)\n",
    "print(yhat)\n",
    "'''Note that the feature map has six elements, whereas our input has eight elements.\n",
    "We can use padding to get 8 elements feature map.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Note:\n",
    "The first dimension refers to each input sample; in this case, we only have one sample.\n",
    "The second dimension refers to the length of each sample; in this case, the length is eight. \n",
    "The third dimension refers to the number of channels in each sample; in this case, we only have a single channel.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two-Dimensional input data example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[[[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]]],\n",
      "\n",
      "\n",
      "       [[[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]]],\n",
      "\n",
      "\n",
      "       [[[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]]]], dtype=float32), array([0.], dtype=float32)]\n",
      "[0.0, 0.0, 3.0, 3.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 3.0, 3.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 3.0, 3.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 3.0, 3.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 3.0, 3.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 3.0, 3.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D\n",
    "# define input data\n",
    "# [1,8,8,1]=>[samples, columns, rows, channels] \n",
    "data = [[0, 0, 0, 1, 1, 0, 0, 0],\n",
    "        [0, 0, 0, 1, 1, 0, 0, 0],\n",
    "        [0, 0, 0, 1, 1, 0, 0, 0],\n",
    "        [0, 0, 0, 1, 1, 0, 0, 0],\n",
    "        [0, 0, 0, 1, 1, 0, 0, 0],\n",
    "        [0, 0, 0, 1, 1, 0, 0, 0],\n",
    "        [0, 0, 0, 1, 1, 0, 0, 0],\n",
    "        [0, 0, 0, 1, 1, 0, 0, 0]]\n",
    "data = asarray(data)\n",
    "data = data.reshape(1, 8, 8, 1)\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(1, (3,3), input_shape=(8, 8, 1)))\n",
    "# define a vertical line detector\n",
    "detector = [[[[0]],[[1]],[[0]]],\n",
    "            [[[0]],[[1]],[[0]]],\n",
    "            [[[0]],[[1]],[[0]]]]\n",
    "weights = [asarray(detector), asarray([0.0])]\n",
    "# store the weights in the model\n",
    "model.set_weights(weights)\n",
    "# confirm they were stored\n",
    "print(model.get_weights())\n",
    "# apply filter to input data\n",
    "yhat = model.predict(data)\n",
    "for r in range(yhat.shape[1]):\n",
    "    # print each column in the row\n",
    "    print([yhat[0,r,c,0] for c in range(yhat.shape[2])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Conv2D:\n",
    "'''\n",
    "keras.layers.Conv2D(filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, \n",
    "dilation_rate=(1, 1), activation=None, use_bias=True, kernel_initializer='glorot_uniform', \n",
    "bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, \n",
    "activity_regularizer=None, kernel_constraint=None, bias_constraint=None)\n",
    "'''\n",
    "'''\n",
    "When using Conv2D as the first layer in a model, provide the keyword argument:\n",
    "input_shape (tuple of integers, does not include the batch axis)\n",
    "e.g. input_shape=(128, 128, 3) for 128x128 RGB pictures in data_format=\"channels_last\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pooling Layers\n",
    "## 1.Problem of output feature map\n",
    "    Sensitive to location of features. 'local translation invariance', downsize the feature map to make it less sensitive.\n",
    "    Small movements in the position of the feature in the input image will result in a different feature map. This can happen with re-cropping, rotation, shifting, and other minor changes to the input image.\n",
    "## 2. Pooling layer\n",
    "    A pooling layer is a new layer added after the convolutional layer. \n",
    "    Pooling layer will always reduce the size of each feature map by a factor of 2, e.g. each dimension is halved, reducing the number of pixels or values in each feature map to one quarter the size.\n",
    "    For example, a pooling layer applied to a feature map of 6×6 (36 pixels) will result in an output pooled feature map of 3×3 (9 pixels).\n",
    "   ### 2.1 Average Pooling\n",
    "    Calculate the average value for each patch on the feature map.\n",
    "    Average pooling involves calculating the average for each patch of the feature map. This means that each 2×2 square of the feature map is down sampled to the average value in the square.\n",
    "    For example:\n",
    "    [0.0, 0.0, 4.0, 5.0, 0.0, 0.0]\n",
    "    [0.0, 0.0, 4.0, 5.0, 0.0, 0.0]\n",
    "    After average pooling:\n",
    "    [0.0,4.5,0.0]\n",
    "   ### 2.2 Max Pooling\n",
    "    Calculate the maximum value for each patch of the feature map.\n",
    "    For example:\n",
    "    [0.0, 0.0, 4.0, 5.0, 0.0, 0.0]\n",
    "    [0.0, 0.0, 4.0, 5.0, 0.0, 0.0]\n",
    "    After max pooling:\n",
    "    [0.0,5.0,0.0]\n",
    "   ### 2.3 Global Pooling\n",
    "    Global pooling down samples the entire feature map to a single value. \n",
    "    This would be the same as setting the pool_size to the size of the input feature map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully Connected Layers\n",
    "    'Flatten out'\n",
    "    Fully connected layers are the normal flat feed-forward neural network layer.\n",
    "    Fully connected layers are used at the end of the network after feature extraction and consolidation has been performed by the convolutional and pooling layers. They are used to create final non-linear combinations of features and for making predictions by the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout Layers\n",
    "    http://jmlr.org/papers/v15/srivastava14a.html\n",
    "    A single model can be used to simulate having a large number of different network architectures by randomly dropping out nodes during training. This is called dropout and offers a very computationally cheap and remarkably effective regularization method to reduce overfitting and improve generalization error in deep neural networks of all kinds.\n",
    "    \n",
    "## 1.1 Dropout \n",
    "    By dropping a unit out, we mean temporarily removing it from the network, along with all its incoming and outgoing connections\n",
    "    \n",
    "## 1.2 How\n",
    "    Dropout is more effective than other standard computationally inexpensive regularizers, such as weight decay, filter norm constraints and sparse activity regularization. \n",
    "    Dropout may also be combined with other forms of regularization to yield a further improvement.\n",
    "### 1.2.1 Where\n",
    "    Dropout is implemented per-layer in a neural network.\n",
    "    It can be used with most types of layers, such as dense fully connected layers, convolutional layers, and recurrent layers such as the long short-term memory network layer.\n",
    "     It is not used on the output layer.\n",
    "     \n",
    "### 1.2.2 Parameter\n",
    "    A new hyperparameter is introduced that specifies the probability at which outputs of the layer are dropped out, or inversely, the probability at which outputs of the layer are retained. \n",
    "    A common value is: \n",
    "        -a probability of 0.5 for retaining the output of each node in a hidden layer. A good value for dropout in a hidden layer is between 0.5 and 0.8.\n",
    "        -a value close to 1.0, such as 0.8, for retaining inputs from the visible layer. Input layers use a larger dropout rate, such as of 0.8.\n",
    "        \n",
    "### 1.2.3 Grid Search Parameters\n",
    "    Rather than guess at a suitable dropout rate for your network, test different rates systematically.\n",
    "    For example, test values between 1.0 and 0.1 in increments of 0.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding & Stride\n",
    "## 1. What is border effects\n",
    "    Reduction in the size of the input to the feature map is referred to as border effects. It is caused by the interaction of the filter with the border of the image.\n",
    "    This is often not a problem for large images and small filters but can be a problem with small images. \n",
    "    **This can become a problem as we develop very deep convolutional neural network models with tens or hundreds of layers. We will simply run out of data in our feature maps upon which to operate.\n",
    "## 2. Padding\n",
    "    Adding addition of pixels to the edge of the image, value zero value that has no effect with the dot product operation when the filter is applied, is called padding. This will lead to applying a filter to an image is to ensure that each pixel in the image is given an opportunity to be at the center of the filter.\n",
    "    model.add(Conv2D(), padding='same') ==> Adds the padding required to the input image (or feature map) to ensure that the output has the same shape as the input.\n",
    "## 3.Stride\n",
    "    The default stride or strides in two dimensions is (1,1) for the height and the width movement.\n",
    "    The stride can be changed, which has an effect both on how the filter is applied to the image and, in turn, the size of the resulting feature map.\n",
    "\n",
    "<table><tr>\n",
    "    <td> <img src=\"Padding.png\" alt=\"Drawing\" style=\"width: 300px;\"/> </td>\n",
    "    <td>   </td>\n",
    "    <td>   </td>\n",
    "    <td>   </td>\n",
    "    <td>   </td>\n",
    "    <td>   </td>\n",
    "    <td>   </td>\n",
    "    <td> <img src=\"Stride.png\" alt=\"Drawing\" style=\"width: 400px;\"/> </td>\n",
    "</tr><table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Atchitectures introduction\n",
    "    The elements of a convolutional neural network, such as convolutional and pooling layers, are relatively straightforward to understand.\n",
    "    The challenging part of using convolutional neural networks in practice is how to design model architectures that best use these simple elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. LeNet-5\n",
    "    https://ieeexplore.ieee.org/abstract/document/726791\n",
    "    Keywords: 7 layers, MINIST Dataset(32*32,grayscale image), Classification, 10 categories\n",
    "### 1.1 Architecture   \n",
    "    In.MINIST images are 28*28, zero-padded to 32*32.\n",
    "    C1.Convolution layer, 6 filters each with the size of 5×5\n",
    "    S2.average Pooling(subsampling)\n",
    "    C3.Convolution layer, 16 filters with a size of 5×5\n",
    "    S4.average Pooling(subsampling)\n",
    "    C5.Convolution layer, 120 filters with size of 1*1\n",
    "    F6.fully connected layer\n",
    "    Out.fully connected layer\n",
    "<table><tr>\n",
    "    <td> <img src=\"LeNet-5.png\" alt=\"Drawing\" style=\"width: 400px;\"/> </td>\n",
    "    <td><img src=\"LeNet-51.png\" alt=\"Drawing\" style=\"width: 400px;\"/> </td>\n",
    "</tr><table>\n",
    "    \n",
    "### 1.2 Compare to mordern \n",
    "    Compared to modern applications, the number of filters is also small, but the trend of increasing the number of filters with the depth of the network also remains a common pattern in modern usage of the technique.\n",
    "    In modern terminology, the final section of the architecture is often referred to as the classifier, whereas the convolutional and pooling layers earlier in the model are referred to as the feature extractor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. AlexNet\n",
    "    http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks\n",
    "    Keywords: ImageNet, similar to LetNet-5, Larger and Deeper, Stack Converlutional layers directly, Max pooling, ReLU, softmax, Dropout Method, \n",
    "### 2.1 Architecture\n",
    "    In.224*224, 3 channels\n",
    "    C1.Convolution, 96 filters with size of 11*11, stride=4\n",
    "    S2.Max Pooling, 3*3, stride=2\n",
    "    C3.Convolution, 256 filters with size of 5*5, stride=1\n",
    "    S4.Max Pooling, 3*3, stride=2\n",
    "    C5.Convolution, 384 filters with size of 3*3\n",
    "    C6.Convolution, 384 filters with size of 3*3\n",
    "    C7.Convolution, 256 filters with size of 3*3\n",
    "    F8.Fully connected, 4096\n",
    "    F9.Fully connected, 4096\n",
    "    Out.Fully connected, 1000\n",
    "<table><tr>\n",
    "    <td> <img src=\"AlexNet.png\" alt=\"Drawing\" style=\"width: 400px;\"/> </td>\n",
    "    <td><img src=\"AlexNet1.png\" alt=\"Drawing\" style=\"width: 400px;\"/> </td>\n",
    "</tr><table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. GoogLeNet\n",
    "    http://www.image-net.org/challenges/LSVRC/2014/\n",
    "    https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Szegedy_Going_Deeper_With_2015_CVPR_paper.html\n",
    "    Keywors: Inception Modules\n",
    "   \n",
    "### 3.1 Inception Module\n",
    "<img src=\"GoolgeLeNet.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "\n",
    "### 3.2 Architecture\n",
    "    A simplified architecture\n",
    "<img src=\"GoogleNet1.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "<img src=\"GoogleNet2.png\" alt=\"Drawing\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ResNet (Residual Network)\n",
    "    https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\n",
    "    Keywords: 152 layers, skip connections(short cut connections)\n",
    "### 4.1 Residual block\n",
    "    These are simply connections in the network architecture where the input is kept as-is (not weighted) and passed on to a deeper layer, e.g. skipping the next layer.\n",
    "    A residual block is a pattern of two convolutional layers with ReLU activation where the output of the block is combined with the input to the block, e.g. the shortcut connection.\n",
    "<img src=\"VGG vs ResNet.png\" alt=\"Drawing\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
